# Form Processing LLM Configuration
# ==================================
# 用于表单处理和数据提取的 LLM 配置

# LLM Provider: openai, azure_openai, vllm, ollama, anthropic, custom
# 表单处理推荐使用 vLLM 本地部署以提高吞吐量
LLM_PROVIDER=vllm

# Model name - 推荐使用支持结构化输出的模型
LLM_MODEL=Qwen/Qwen2.5-7B-Instruct

# API Configuration
# vLLM 本地部署不需要 API Key
LLM_API_KEY=
LLM_API_BASE=http://localhost:8000/v1

# Generation Parameters - 表单提取需要极低温度保证一致性
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=4096
LLM_TOP_P=1.0

# Request Settings - 批量处理可能需要更长超时
LLM_TIMEOUT=120.0
LLM_MAX_RETRIES=3
